{"meta":{"title":"Hexo","subtitle":"","description":true,"author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"docker配置","slug":"docker配置","date":"2020-09-11T15:50:53.000Z","updated":"2020-09-11T17:13:17.081Z","comments":true,"path":"2020/09/11/docker配置/","link":"","permalink":"http://example.com/2020/09/11/docker%E9%85%8D%E7%BD%AE/","excerpt":"镜像（mac） http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/?spm=5176.8351553.0.0.7f231991HGal1K","text":"镜像（mac） http://mirrors.aliyun.com/docker-toolbox/mac/docker-for-mac/?spm=5176.8351553.0.0.7f231991HGal1K 镜像中心：https://c.163yun.com/hub#/home docker容器中安装vim apt-get install vim apt-get update apt-get install vim 命令 docker images(查看镜像) docker stop id(停止) docker ps docker ps -a(已停止的容器) docker version docker rmi id(删除容器，运行过的需停止后，删除停止后的容器，方可彻底删除) docker container prune (删除所有停止的容器) docker pull xxx(拉去镜像) docker run -p 8080:8080 (前本地后容器端口) -d (后台执行) docker restart docker exec -it id bash(进入容器内部) docker logs id(日志) docker inspect id(查看挂载情况等) docker run -p 8080:8080 -v /path:/path -v /path:/path id (挂载，前者本地后者容器)","categories":[],"tags":[]},{"title":"jenkins配置","slug":"jenkins配置","date":"2020-09-11T15:49:53.000Z","updated":"2020-09-11T17:03:06.279Z","comments":true,"path":"2020/09/11/jenkins配置/","link":"","permalink":"http://example.com/2020/09/11/jenkins%E9%85%8D%E7%BD%AE/","excerpt":"地址（国内仓库） https://mirrors.tuna.tsinghua.edu.cn/jenkins/war/","text":"地址（国内仓库） https://mirrors.tuna.tsinghua.edu.cn/jenkins/war/ 执行 java -jar jenkins.war –httpPort=8899","categories":[],"tags":[]},{"title":"加密算法","slug":"加密算法","date":"2020-09-10T15:08:26.000Z","updated":"2020-09-10T15:58:41.968Z","comments":true,"path":"2020/09/10/加密算法/","link":"","permalink":"http://example.com/2020/09/10/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/","excerpt":"什么是加密算法 数据加密的基本过程就是对原来为明文的文件或数据按某种算法进行处理，使其成为不可读的一段代码为“密文”，使其只能在输入相应的密钥之后才能显示出原容，通过这样的途径来达到保护数据不被非法人窃取、阅读的目的。 该过程的逆过程为解密，即将该编码信息转化为其原来数据的过程。","text":"什么是加密算法 数据加密的基本过程就是对原来为明文的文件或数据按某种算法进行处理，使其成为不可读的一段代码为“密文”，使其只能在输入相应的密钥之后才能显示出原容，通过这样的途径来达到保护数据不被非法人窃取、阅读的目的。 该过程的逆过程为解密，即将该编码信息转化为其原来数据的过程。 分类 对称加密 对称式加密就是加密和解密使用同一个密钥 DES（Data Encryption Standard） 其入口参数有三个：key、data、mode。key为加密解密使用的密钥，data为加密解密的数据，mode为其工作模式。当模式为加密模式时，明文按照64位进行分组，形成明文组，key用于对数据加密，当模式为解密模式时，key用于对数据解密。实际运用中，密钥只用到了64位中的56位，这样才具有高的安全性。 置换表：把输入的64位数据块按位重新组合，并把输出分为L0、R0两部分，每部分各长32位 58,50,42,34,26,18,10,2,60,52,44,36,28,20,12,4, 62,54,46,38,30,22,14,6,64,56,48,40,32,24,16,8, 57,49,41,33,25,17,9,1,59,51,43,35,27,19,11,3, 61,53,45,37,29,21,13,5,63,55,47,39,31,23,15,7, 将输入的第58位换到第一位，第50位换到第2位，…，依此类推，最后一位是原来的第7位：输入值为D1D2D3……D64，L0=D58D50…D8；R0=D57D49…D7 未完待续 非对称加密 非对称式加密就是加密和解密所使用的不是同一个密钥，通常有两个密钥，称为“公钥”和“私钥”，它们两个必需配对使用，否则不能打开加密文件 不可逆加密 算法 一个加密系统S可以用数学符号描述如下：S={P, C, K, E, D} P——明文空间，表示全体可能出现的明文集合 C——密文空间，表示全体可能出现的密文集合 K——密钥空间，密钥是加密算法中的可变参数 E——加密算法，由一些公式、法则或程序构成 D——解密算法，它是E的逆 关系：（密钥为k） C = Ek(P), 对明文P加密后得到密文C P = Dk(C) = Dk(Ek(P)), 对密文C解密后得明文P 如用E-1 表示E的逆，D-1表示D的逆，则有：Ek = Dk-1且Dk = Ek-1","categories":[],"tags":[]},{"title":"springboot+shiro+jwt+mybatis整合","slug":"springboot+mybatis+shiro+jwt","date":"2020-02-05T14:56:08.000Z","updated":"2020-09-14T14:59:14.718Z","comments":true,"path":"2020/02/05/springboot+mybatis+shiro+jwt/","link":"","permalink":"http://example.com/2020/02/05/springboot+mybatis+shiro+jwt/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;maven-4.0.0.xsd&quot;&gt;&lt;!-- more --&gt; &lt;parent&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;version&gt;2.3.3.RELEASE&lt;&#x2F;version&gt; &lt;&#x2F;parent&gt; &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt; &lt;artifactId&gt;application-car&lt;&#x2F;artifactId&gt; &lt;groupId&gt;org.example&lt;&#x2F;groupId&gt; &lt;packaging&gt;pom&lt;&#x2F;packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt; &lt;modules&gt; &lt;module&gt;api&lt;&#x2F;module&gt; &lt;module&gt;db&lt;&#x2F;module&gt; &lt;module&gt;service&lt;&#x2F;module&gt; &lt;module&gt;controller&lt;&#x2F;module&gt; &lt;module&gt;common&lt;&#x2F;module&gt; &lt;&#x2F;modules&gt; &lt;properties&gt; &lt;alibaba.fastjson.version&gt;1.2.4&lt;&#x2F;alibaba.fastjson.version&gt; &lt;jjwt.version&gt;0.9.0&lt;&#x2F;jjwt.version&gt; &lt;jwt.auth0.version&gt;3.2.0&lt;&#x2F;jwt.auth0.version&gt; &lt;spring.shiro.version&gt;1.4.0&lt;&#x2F;spring.shiro.version&gt; &lt;dubbo.version&gt;2.6.3&lt;&#x2F;dubbo.version&gt; &lt;mysql.version&gt;5.1.30&lt;&#x2F;mysql.version&gt; &lt;alibaba.druid.version&gt;1.1.10&lt;&#x2F;alibaba.druid.version&gt; &lt;spring.mybatis.verssion&gt;1.3.1&lt;&#x2F;spring.mybatis.verssion&gt; &lt;&#x2F;properties&gt; &lt;dependencies&gt; &lt;!--旧项目--&gt; &lt;dependency&gt; &lt;groupId&gt;com.datacenter.jeefp&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jeefp-single&lt;&#x2F;artifactId&gt; &lt;version&gt;1.2.8&lt;&#x2F;version&gt; &lt;scope&gt;compile&lt;&#x2F;scope&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;&#x2F;artifactId&gt; &lt;exclusions&gt;&lt;!-- 去掉springboot默认配置 --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;&#x2F;artifactId&gt; &lt;&#x2F;exclusion&gt; &lt;&#x2F;exclusions&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;!-- 引入log4j2依赖 --&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;fastjson&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba.fastjson.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--jwt+shiro--&gt; &lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jjwt&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;jjwt.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;&#x2F;groupId&gt; &lt;artifactId&gt;java-jwt&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;jwt.auth0.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;&#x2F;groupId&gt; &lt;artifactId&gt;shiro-spring&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;spring.shiro.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!--druid连接池--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba.druid.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;&#x2F;groupId&gt; &lt;artifactId&gt;druid&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;alibaba.druid.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- orm-mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;spring.mybatis.verssion&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;!-- orm-jooq --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt; &lt;artifactId&gt;spring-boot-starter-jooq&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;&#x2F;groupId&gt; &lt;artifactId&gt;jedis&lt;&#x2F;artifactId&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt;&lt;&#x2F;project&gt; 12345678server: port: 8084spring: profiles: active: - devmybatis: mapper-locations: classpath:com&#x2F;datacenter&#x2F;jeefpy&#x2F;mapper&#x2F;**&#x2F;*.xml 123456789101112131415161718192021222324spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql:&#x2F;&#x2F;:3306&#x2F;qyerp-test?characterEncoding&#x3D;UTF-8 username: password: type: com.alibaba.druid.pool.DruidDataSource druid: max-active: 20 initial-size: 5 min-idle: 5 max-wait: 60000 time-between-eviction-runs-millis: 60000 min-evictable-idle-time-millis: 300000 test-while-idle: true test-on-borrow: false test-on-return: false redis: host: 127.0.0.1 port: 6379 maxTotal: 100 maxIdle: 10 maxWaitMillis: 100000 password: 123456","categories":[],"tags":[]},{"title":"log4j2日志全解","slug":"log4j2日志全解","date":"2020-01-06T12:01:24.000Z","updated":"2020-09-10T15:06:41.525Z","comments":true,"path":"2020/01/06/log4j2日志全解/","link":"","permalink":"http://example.com/2020/01/06/log4j2%E6%97%A5%E5%BF%97%E5%85%A8%E8%A7%A3/","excerpt":"常用日志框架 java.util.logging：是JDK在1.4版本中引入的Java原生日志框架","text":"常用日志框架 java.util.logging：是JDK在1.4版本中引入的Java原生日志框架 Log4j：Apache的一个开源项目，可以控制日志信息输送的目的地是控制台、文件、GUI组件等，可以控制每一条日志的输出格式，这些可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。虽然已经停止维护了，但目前绝大部分企业都是用的log4j。 LogBack：是Log4j的一个改良版本 Log4j2：Log4j2已经不仅仅是Log4j的一个升级版本了，它从头到尾都被重写了 日志门面slf4j 上述介绍的是一些日志框架的实现，这里我们需要用日志门面来解决系统与日志实现框架的耦合性。SLF4J，即简单日志门面（Simple Logging Facade for Java），它不是一个真正的日志实现，而是一个抽象层（ abstraction layer），它允许你在后台使用任意一个日志实现。 前面介绍的几种日志框架一样，每一种日志框架都有自己单独的API，要使用对应的框架就要使用其对应的API，这就大大的增加应用程序代码对于日志框架的耦合性。 使用了slf4j后，对于应用程序来说，无论底层的日志框架如何变，应用程序不需要修改任意一行代码，就可以直接上线了。 为什么选用log4j2 相比与其他的日志系统，log4j2丢数据这种情况少；disruptor技术，在多线程环境下，性能高于logback等10倍以上；利用jdk1.5并发的特性，减少了死锁的发生； log4j2优越的性能其原因在于log4j2使用了LMAX,一个无锁的线程间通信库代替了,logback和log4j之前的队列. 并发性能大大提升。 springboot springboot默认是用logback的日志框架的，所以需要排除logback，不然会出现jar依赖冲突的报错。 参考文献 https://zhuanlan.zhihu.com/p/70090008?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=984232333343399936&amp;utm_campaign=shareopn","categories":[],"tags":[]},{"title":"Mysql索引优化及原理","slug":"Mysql索引分析及原理","date":"2020-01-04T13:01:24.000Z","updated":"2020-09-10T15:05:34.530Z","comments":true,"path":"2020/01/04/Mysql索引分析及原理/","link":"","permalink":"http://example.com/2020/01/04/Mysql%E7%B4%A2%E5%BC%95%E5%88%86%E6%9E%90%E5%8F%8A%E5%8E%9F%E7%90%86/","excerpt":"Mark B+Tree 可以对 &lt;，&lt;=，=，&gt;，&gt;=，BETWEEN？，IN？，以及不以通配符开始的 LIKE 使用索引。（MySQL 5.5 后） 索引的优缺点 索引的优点如下： 索引大大减小了服务器需要扫描的数据量。","text":"Mark B+Tree 可以对 &lt;，&lt;=，=，&gt;，&gt;=，BETWEEN？，IN？，以及不以通配符开始的 LIKE 使用索引。（MySQL 5.5 后） 索引的优缺点 索引的优点如下： 索引大大减小了服务器需要扫描的数据量。 索引可以帮助服务器避免排序和临时表。 索引可以将随机 I/O 变成顺序 I/O。 索引的缺点如下： 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE 和 DELETE。因为更新表时，MySQL 不仅要保存数据，还要保存索引文件。 建立索引会占用磁盘空间的索引文件。一般情况这个问题不算严重，但如果你在一个大表上创建了多种组合索引，且伴随大量数据量插入，索引文件大小也会快速膨胀。 如果某个数据列包含许多重复的内容，为它建立索引就没有太大的实际效果。 对于非常小的表，大部分情况下简单的全表扫描更高效。 因此应该只为最经常查询和最经常排序的数据列建立索引。（MySQL 里同一个数据表里的索引总数限制为 16 个） 预读 磁盘读取完需要的数据后，会按顺序再多读一部分数据到内存中，这样做的理论依据是计算机科学中注明的局部性原理： 由于磁盘顺序读取的效率很高（不需要寻址时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高 I/O 效率。预读的长度一般为页（page）的整倍数。 MySQL（默认使用 InnoDB 引擎），将记录按照页的方式进行管理，每页大小默认为 16K（可以修改）。 B-Tree 借助计算机磁盘预读机制：每次新建节点的时候，都是申请一个页的空间，所以每查找一个节点只需要一次 I/O；因为实际应用当中，节点深度会很少，所以查找效率很高。 为什么 MySQL 索引选择了 B+树而不是 B 树？ MySQL 是关系型数据库，经常会按照区间来访问某个索引列，B+树的叶子节点间按顺序建立了链指针，加强了区间访问性，所以 B+树对索引列上的区间范围查询很友好。而 B 树每个节点的 key 和 data 在一起，无法进行区间查找。 案例 回表查询 比如你创建了 name， age 索引 name_age_index，查询数据时使用了：select * from table where name =’陈哈哈’ and age = 26; 由于附加索引中只有 name 和 age，因此命中索引后，数据库还必须回去聚集索引中查找其他数据，这就是回表，这也是你背的那条：少用 select * 的原因。 索引覆盖 结合回表会更好理解，比如上述 name_age_index 索引，有查询：select name， age from table where name =’陈哈哈’ and age = 26; 最左前缀原则 B+树的节点存储索引顺序是从左向右存储，在匹配的时候自然也要满足从左向右匹配。 比如索引 abc_index：（a，b，c）是 a，b，c 三个字段的联合索引，下列 sql 执行时都无法命中索引 abc_index 的。 select * from table where c = ‘1’; select * from table where b =’1’ and c =’2’; 以下三种情况却会走索引： select * from table where a = ‘1’; select * from table where a = ‘1’ and b = ‘2’; select * from table where a = ‘1’ and b = ‘2’ and c=’3’; 其实（a，c）也会走，但是只走 a 字段索引，不会走 c 字段。 最左前缀 顾名思义，就是最左优先，上例中我们创建了 a_b_c 多列索引，相当于创建了（a）单列索引，（a，b）组合索引以及（a，b，c）组合索引。 因此，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边。 索引下推优化 select * from table where name like ‘陈%’ and age &gt; 26； 命中 name_age_index 联合索引，查询所有满足 name 以”陈”开头的数据， 然后回表查询所有满足的行。（先查出陈后，age可能为乱序） 命中 name_age_index 联合索引，查询所有满足 name 以”陈”开头的数据，然后顺便筛出 age&gt;20 的索引，再回表查询全行数据。 显然第 2 种方式回表查询的行数较少，I/O 次数也会减少，这就是索引下推。所以不是所有 like 都不会命中索引。","categories":[],"tags":[]},{"title":"CLH锁","slug":"CLH锁","date":"2020-01-02T10:56:08.000Z","updated":"2020-09-07T14:21:00.090Z","comments":true,"path":"2020/01/02/CLH锁/","link":"","permalink":"http://example.com/2020/01/02/CLH%E9%94%81/","excerpt":"什么是自旋锁和互斥锁？","text":"什么是自旋锁和互斥锁？ 自旋锁 自旋锁说白了也是一种互斥锁，只不过没有抢到锁的线程会一直自旋等待锁的释放，处于busy-waiting的状态，此时等待锁的线程不会进入休眠状态，而是一直忙等待浪费CPU周期。因此自旋锁适用于锁占用时间短的场合。 互斥锁 多个线程并发竞争锁的时候，没有抢到锁的线程会进入休眠状态即sleep-waiting 当锁被释放的时候，处于休眠状态的一个线程会再次获取到锁。缺点就是这一些列过程需要线程切换，需要执行很多CPU指令，同样需要时间。如果CPU执行线程切换的时间比锁占用的时间还长，那么可能还不如使用自旋锁。因此互斥锁适用于锁占用时间长的场合。 差异：自旋锁(定时器时刻监听)，互斥锁(锁释放后通知，线程切换的开销) 什么是CLH锁？ CLH锁其实就是一种是基于逻辑队列非线程饥饿的一种自旋公平锁，由于是 Craig、Landin 和 Hagersten三位大佬的发明，因此命名为CLH锁。 未完待续","categories":[],"tags":[]},{"title":"模拟高并发(1)","slug":"模拟高并发(1)","date":"2019-12-03T15:56:08.000Z","updated":"2020-09-07T13:54:05.871Z","comments":true,"path":"2019/12/03/模拟高并发(1)/","link":"","permalink":"http://example.com/2019/12/03/%E6%A8%A1%E6%8B%9F%E9%AB%98%E5%B9%B6%E5%8F%91(1)/","excerpt":"事例","text":"事例 public static void access() { if(map.containsKey(true)){ System.out.println(“访问了redis”); }else { lock.lock(); if(map.containsKey(true)) { System.out.println(“访问了redis”); }else { System.out.println(“访问了db”); map.put(true, “adf”); } lock.unlock(); } }","categories":[],"tags":[]},{"title":"oAuth","slug":"oAuth","date":"2019-11-07T12:34:10.000Z","updated":"2020-09-06T06:15:19.991Z","comments":true,"path":"2019/11/07/oAuth/","link":"","permalink":"http://example.com/2019/11/07/oAuth/","excerpt":"oAuth简介 OAUTH协议为用户资源的授权提供了一个安全的、开放而又简易的标准。与以往的授权方式不同之处是OAUTH的授权不会使第三方触及到用户的帐号信息（如用户名与密码），即第三方无需使用用户的用户名与密码就可以申请获得该用户资源的授权，因此OAUTH是安全的。oAuth是Open Authorization的简写。","text":"oAuth简介 OAUTH协议为用户资源的授权提供了一个安全的、开放而又简易的标准。与以往的授权方式不同之处是OAUTH的授权不会使第三方触及到用户的帐号信息（如用户名与密码），即第三方无需使用用户的用户名与密码就可以申请获得该用户资源的授权，因此OAUTH是安全的。oAuth是Open Authorization的简写。 产生背景 典型案例：如果一个用户需要两项服务：一项服务是图片在线存储服务A，另一个是图片在线打印服务B。由于服务A与服务B是由两家不同的服务提供商提供的，所以用户在这两家服务提供商的网站上各自注册了两个用户，假设这两个用户名各不相同，密码也各不相同。当用户要使用服务B打印存储在服务A上的图片时，用户该如何处理? 法一：用户可能先将待打印的图片从服务A上下载下来并上传到服务B上打印，这种方式安全但处理比较繁琐，效率低下 法二：用户将在服务A上注册的用户名与密码提供给服务B，服务B使用用户的帐号再去服务A处下载待打印的图片，这种方式效率是提高了，但是安全性大大降低了，服务B可以使用用户的用户名与密码去服务A上查看甚至篡改用户的资源。 解决办法（oAuth） 如：举个例子，你想登录豆瓣去看看电影评论，但你丫的从来没注册过豆瓣账号，又不想新注册一个再使用豆瓣，怎么办呢？不用担心，豆瓣已经为你这种懒人做了准备，用你的qq号可以授权给豆瓣进行登录 第一步：在豆瓣官网点击用qq登录 第二步：跳转到qq登录页面输入用户名密码，然后点授权并登录（拿到code） 第三步：跳回到豆瓣页面，成功登录（模拟浏览器发起了两次请求。一个是用拿到的code去换token，另一个就是用拿到的token换取用户信息。最后将用户信息储存起来（db），返回给浏览器其首页的视图）","categories":[],"tags":[]},{"title":"多线程小知识点(1)","slug":"多线程小知识点(1)","date":"2019-11-03T10:56:08.000Z","updated":"2020-09-07T13:53:50.033Z","comments":true,"path":"2019/11/03/多线程小知识点(1)/","link":"","permalink":"http://example.com/2019/11/03/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9(1)/","excerpt":"Thread.join 使用Thread的join()等待所有的子线程执行完毕，主线程再执行，thread.join()把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B","text":"Thread.join 使用Thread的join()等待所有的子线程执行完毕，主线程再执行，thread.join()把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B CountDownLatch run()方法中调用CountDownLatch.countDown(); 需要子线程全部执行完再执行的主线程CountDownLatch.await(),原理：阻塞当前线程直到变量CountDownLatch为0 CyclicBarrier CyclicBarrier适用于这样的情况：你希望创建一组任务，他们并行地执行工作，然后在进行的下一个步骤前等待，直至所有任务都完成(有点像join())。它使得所有的并行任务都在栅栏处列队。（先CyclicBarrier.await()的处于第一位，其他线程需等待它执行完毕） countDownLatch和cyclicBarrier区别 countDownLatch只能使用一次，而CyclicBarrier方法可以使用reset()方法重置，所以CyclicBarrier方法可以能处理更为复杂的业务场景 百米赛跑的比赛中若使用 countDownLatch的话冲过终点线一个人就给评委发送一个人的成绩(CountDownLatch.countDown())，10个人比赛发送10次，如果用CyclicBarrier，则只在最后一个人冲过终点线的时候发送所有人的数据，仅仅发送一次，这就是区别(到达阈值时执行最后的操作，未到达会阻塞)","categories":[],"tags":[]},{"title":"乐观锁和悲观锁","slug":"乐观锁和悲观锁","date":"2019-09-12T13:50:09.000Z","updated":"2020-09-05T15:57:30.658Z","comments":true,"path":"2019/09/12/乐观锁和悲观锁/","link":"","permalink":"http://example.com/2019/09/12/%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81/","excerpt":"何谓悲观锁与乐观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人","text":"何谓悲观锁与乐观锁 乐观锁对应于生活中乐观的人总是想着事情往好的方向发展，悲观锁对应于生活中悲观的人总是想着事情往坏的方向发展。这两种人各有优缺点，不能不以场景而定说一种人好于另外一种人 乐观锁 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的 悲观锁 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适 cas在多写情况下自旋：非公平锁和公平锁（队列维护） CAS算法 即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。 CAS（总读取主内存的值） CAS与synchronized的使用情景 简单的来说CAS适用于写比较少的情况下（多读场景，冲突一般较少），synchronized适用于写比较多的情况下（多写场景，冲突一般较多）","categories":[],"tags":[]},{"title":"高并发的理解和使用场景","slug":"高并发的理解和使用场景","date":"2019-09-12T12:52:10.000Z","updated":"2020-09-05T15:49:17.971Z","comments":true,"path":"2019/09/12/高并发的理解和使用场景/","link":"","permalink":"http://example.com/2019/09/12/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%9A%84%E7%90%86%E8%A7%A3%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"高并发的理解 就是短时间内遇到大量操作请求，导致站点服务器/db服务器资源被占满甚至严重时直接导致宕","text":"高并发的理解 就是短时间内遇到大量操作请求，导致站点服务器/db服务器资源被占满甚至严重时直接导致宕 衡量一个系统的好坏，除了业务外，还有就是系统的吞吐量（单位时间内处理的请求数）—–QPS（每秒钟能处理的请求数）和响应时间 高并发和多线程的关系 多线程的理解 多线程是java的特性，因为现在cpu都是多核多线程的，可以同时执行几个任务，为了提高jvm的执行效率，java提供了这种多线程的机制，以增强数据处理效率 多线程对应的是cpu，高并发对应的是访问请求，可以用单线程处理所有访问请求，也可以用多线程同时处理访问请求 在过去单CPU时代，单任务在一个时间点只能执行单一程序。之后发展到多任务阶段，计算机能在同一时间点并行执行多任务或多进程。虽然并不是真正意义上的“同一时间点”，而是多个任务或进程共享一个CPU，并交由操作系统来完成多任务间对CPU的运行切换，以使得每个任务都有机会获得一定的时间片运行 再后来发展到多线程技术，使得在一个程序内部能拥有多个线程并行执行。一个线程的执行可以被认为是一个CPU在执行该程序。当一个程序运行在多线程下，就好像有多个CPU在同时执行该程序 并发编程的几个要素 原子性原子，即一个不可再被分割的颗粒。在Java中原子性指的是一个或多个操作要么全部执行成功要么全部执行失败 有序性程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序） 可见性当多个线程访问同一个变量时，如果其中一个线程对其作了修改，其他线程能立即获取到最新的值 高并发的场景 一般像火车票抢票，秒杀 系统，双11或者京东618活动等这种太明显不过了，这种还是正常的业务范围，蛮好理解的，还有一种就是恶意的攻击，导致系统的某个功能近乎瘫痪，比如验证码的请求等 高并发难点 并发时最怕的就是对共享变量的同时访问导致脏数据的产生，所以一般会加锁：对象锁（例如：syncrinized等关键字）和 分布式锁（数据库锁，redis，zookeeper） 对象锁顾名思义就是锁住当前对象–只能用在单服务器上，对于分布式系统或者单系统分布式部署时对共享资源的访问就必须使用分布式锁了，此时对象锁没法用了 像秒杀系统可以使用缓存让还有数量时都可以看到，而在开抢后得看个人运气了（网络等原因），此时使用乐观锁（共享锁）就搞定了嘛 像银行的消费后更新银行卡余额，使用悲观锁（排斥锁）就可以","categories":[],"tags":[]},{"title":"Redis内存淘汰策略","slug":"Redis内存淘汰策略","date":"2019-08-05T14:56:08.000Z","updated":"2020-09-05T15:57:19.362Z","comments":true,"path":"2019/08/05/Redis内存淘汰策略/","link":"","permalink":"http://example.com/2019/08/05/Redis%E5%86%85%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/","excerpt":"为什么需要内存淘汰策略 长期将Redis作为缓存使用，难免会遇到内存空间存储瓶颈，当Redis内存超出物理内存限制时，内存数据就会与磁盘产生频繁交换，使Redis性能急剧下降。","text":"为什么需要内存淘汰策略 长期将Redis作为缓存使用，难免会遇到内存空间存储瓶颈，当Redis内存超出物理内存限制时，内存数据就会与磁盘产生频繁交换，使Redis性能急剧下降。 使用配置(针对redis.conf) maxmemory 100mb (最大内存,0表示不限制) maxmemory-policy (选择策略) 内存淘汰策略工作流程 客户端会发起需要更多内存的申请 Redis检查内存使用情况，如果实际使用内存已经超出maxmemory，Redis就会根据用户配置的淘汰策略选出无用的key 确认选中数据没有问题，成功执行淘汰任务 淘汰策略 volatile-lru 从设置过期时间的数据集中挑选出最近最少使用的数据淘汰。没有设置过期时间的key不会被淘汰，这样就可以在增加内存空间的同时保证需要持久化的数据不会丢失 volatile-ttl 只从设置失效（expire set）的key中，选出存活时间（TTL）最短的key进行删除，用以保存新数据 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰。当内存达到限制无法写入非过期时间的数据集时，可以通过该淘汰策略在主键空间中随机移除某个key allkeys-lru 从数据集中挑选最近最少使用的数据淘汰，该策略要淘汰的key面向的是全体key集合，而非过期的key集合 allkeys-random 从数据集中选择任意数据淘汰 no-enviction 禁止驱逐数据，也就是当内存不足以容纳新入数据时，新写入操作就会报错，请求可以继续进行，线上任务也不能持续进行，采用no-enviction策略可以保证数据不被丢失，这也是系统默认的一种淘汰策略 LRU淘汰机制 服务器配置中保存了 lru 计数器 server.lrulock，会定时（redis 定时程序 serverCorn()）更新，server.lrulock 的值是根据 server.unixtime 计算出来进行排序的，然后选择最近使用时间最久的数据进行删除。另外，从 struct redisObject 中可以发现，每一个 redis 对象都会设置相应的 lru。每一次访问数据，会更新对应redisObject.lru 在Redis中，LRU算法是一个近似算法，默认情况下，Redis会随机挑选5个键，并从中选择一个最久未使用的key进行淘汰。在配置文件中，按maxmemory-samples选项进行配置，选项配置越大，消耗时间就越长，但结构也就越精准(近似非绝对) TTL淘汰 Redis 数据集数据结构中保存了键值对过期时间的表，即 redisDb.expires。与 LRU 数据淘汰机制类似，TTL 数据淘汰机制中会先从过期时间的表中随机挑选几个键值对，取出其中 ttl 最大的键值对淘汰。同样，TTL淘汰策略并不是面向所有过期时间的表中最快过期的键值对，而只是随机挑选的几个键值对(近似非绝对) 随机淘汰 在随机淘汰的场景下获取待删除的键值对，随机找hash桶再次hash指定位置的dictEntry(指针指向下一个即可:链表)","categories":[],"tags":[]}],"categories":[],"tags":[]}